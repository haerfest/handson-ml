{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Build your own CNN and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "As per [Caffe2's MNIST tutorial](https://caffe2.ai/docs/tutorial-MNIST.html), we are going to build a small CNN based on the LeNet architecture:\n",
    "\n",
    "Layer  | Type            | Maps | Size    | Kernel size | Stride | Activation\n",
    "-------|-----------------|------|---------|-------------|--------|-----------\n",
    "logits | Fully Connected | -    | 10      | -           | -      | softmax\n",
    "F5     | Fully Connected | -    | 500     | -           | -      | relu\n",
    "S4     | Max Pooling     | 100  | 4 x 4   | 2 x 2       | 2      | -\n",
    "C3     | Convolution     | 100  | 8 x 8   | 5 x 5       | 1      | tanh\n",
    "S2     | Max Pooling     | 20   | 12 x 12 | 2 x 2       | 2      | -\n",
    "C1     | Convolution     | 20   | 28 x 28 | 5 x 5       | 1      | tanh\n",
    "X      | Input           | 1    | 28 x 28 | -           | -      | -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the data we will be feeding the network at each training step:\n",
    "\n",
    "- `X` is a batch of 28 x 28 grayscale (= 1 channel) images. The MNIST dataset contains them as a sequence of 768 pixel values, so we'll first reshape them to a 2D image.\n",
    "- `y` is a batch of [one-hot encoded](https://en.wikipedia.org/wiki/One-hot) digit classes.\n",
    "\n",
    "The one-hot encoding simply means that each digit class, which lies in the range 0-9, is represented by a sequence of ten values, all zero except for the index corresponding to the digit class itself.\n",
    "\n",
    "For instance, the digit class 3's one-hot encoding is `0001000000`, or:\n",
    "\n",
    "`i`    | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
    "-------|---|---|---|---|---|---|---|---|---|---|\n",
    "`y[i]` | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28 * 28 * 1])\n",
    "y = tf.placeholder(tf.int32,   shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the model according to the table above. This should be fairly straightforward, except perhaps for the last layer.\n",
    "\n",
    "The last layer of the network contains ten output neurons. Like every neuron in the network, they multiply their inputs by their weights and add a bias term:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\sum{Wx} + b\n",
    "\\end{equation}\n",
    "\n",
    "But note that they have no activation function. This means that they pass on the result of this calculation to their outputs as-is, and do not first perform a _tanh_ or _relu_ conversion. These output values are therefore not limited to a fixed range, such as [0, 1], but can take on any real number, both positive and negative.\n",
    "\n",
    "Now, we want to teach each output neuron to output a higher value when it thinks the input image contains \"its\" digit, and a lower value when it does not.\n",
    "\n",
    "## Logits and probabilities\n",
    "\n",
    "We therefore treat these output values as [logits](https://stats.stackexchange.com/questions/52825/what-does-the-logit-value-actually-mean#52836), or _logarithmic odds_. Given a probability $p$, its odds is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p}{1 - p}\n",
    "\\end{equation}\n",
    "\n",
    "And a logarithmic odd is simply the natural logarithm of this:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\ln \\frac{p}{1 - p}\n",
    "\\end{equation}\n",
    "\n",
    "And the other way around:\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{1}{1 + e^{-L}}\n",
    "\\end{equation}\n",
    "\n",
    "The relationship between a probability $p$ and the logit $L$ can be shown graphically as well:\n",
    "\n",
    "![Curve](https://i.stack.imgur.com/h6N7o.png)\n",
    "\n",
    "The thing to take away is we want to teach an output neuron to output a higher _logit_ when it thinks the input image contains \"its\" digit, since a higher logit corresponds to a higher _probability_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cnn'):\n",
    "\n",
    "    # Image: [768] --> [28 x 28 x 1].\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # Image: [28 x 28 x 1] --> [24 x 24 x 20].\n",
    "    c1 = tf.layers.conv2d(\n",
    "        X_reshaped,\n",
    "        filters=20,\n",
    "        kernel_size=[5, 5],\n",
    "        strides=[1, 1],\n",
    "        padding='valid',\n",
    "        activation=tf.nn.tanh)\n",
    "\n",
    "    # Image: [24 x 24 x 20] --> [12 x 12 x 20].\n",
    "    s2 = tf.layers.max_pooling2d(\n",
    "        c1,\n",
    "        pool_size=[2, 2],\n",
    "        strides=[2, 2],\n",
    "        padding='valid')\n",
    "\n",
    "    # Image: [12 x 12 x 20] --> [8 x 8 x 100].\n",
    "    c3 = tf.layers.conv2d(\n",
    "        s2,\n",
    "        filters=100,\n",
    "        kernel_size=[5, 5],\n",
    "        strides=[1, 1],\n",
    "        padding='valid',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Image: [8 x 8 x 100] --> [4 x 4 x 100].\n",
    "    s4 = tf.layers.max_pooling2d(\n",
    "        c3,\n",
    "        pool_size=[2, 2],\n",
    "        strides=[2, 2],\n",
    "        padding='valid')\n",
    "\n",
    "    # Image: [4 x 4 x 100] --> [1600].\n",
    "    s4_reshaped = tf.reshape(s4, [-1, 4 * 4 * 100])\n",
    "\n",
    "    # Image: [1600] --> [500].\n",
    "    f5 = tf.layers.dense(\n",
    "        s4_reshaped,\n",
    "        units=500,\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Image: [500] --> [10]\n",
    "    logits = tf.layers.dense(\n",
    "        f5,\n",
    "        units=10,\n",
    "        activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn something, we must first define a loss or cost function. For each image we feed the network, the loss tells us in essence how close to the right answer we are, or how far away.\n",
    "\n",
    "The larger the difference between the answer given by the network and the desired answer, that is, the ground truth, the higher the loss value. While learning we aim to minimize the loss to as close to zero as possible.\n",
    "\n",
    "Recall that when we feed the network a single image, we end up with ten _logits_ at the output end of the network. We also have a one-hot encoded sequence of ten values that indicate the desired output. How do we calculate a loss given this combination?\n",
    "\n",
    "## Softmax\n",
    "\n",
    "We first apply the _softmax_ function to the ten outputs. This function takes the ten outputs, and normalizes each output value to the _probability_ that the input image belongs to \"its\" class. The formula is quite simple:\n",
    "\n",
    "\\begin{equation}\n",
    "p_i = \\frac{e^{L_i}}{\\sum_{j=0}^9 e^{L_j}}\n",
    "\\end{equation}\n",
    "\n",
    "This means that for an output neuron at index $i$, it divides the exponential of its (logit) output $L_i$ by the sum of the exponentials of the (logit) outputs of all output neurons.\n",
    "\n",
    "This action normalizes each output value to a probability in the range [0, 1], and has the property that the sum of all probabilities is 1.\n",
    "\n",
    "So far, so good. We now have converted the logit values of the ten output neurons to ten probabilities. We now need to define a loss function that will output a low value if the network guesses correctly on an image, that is, when the network assignes a high probability to the right digit class, and a low probability to all others.\n",
    "\n",
    "## Cross entropy\n",
    "\n",
    "The _cross entropy_ formula is typically used in this case. The cross entropy function looks like this, for a single image:\n",
    "\n",
    "\\begin{equation}\n",
    "J = - \\sum_{i=0}^9 y_i \\log p_i\n",
    "\\end{equation}\n",
    "\n",
    "Here $y_i$ is a value in the [one-hot encoded](https://en.wikipedia.org/wiki/One-hot) label `y`, meaning it's 1 if the image's digit class is $i$, and 0 everywhere else. The $p_i$ probability is what we just calculated earlier using _softmax_ and is in the range [0, 1].\n",
    "\n",
    "Suppose the network guessed the digit 3 correctly by assigning probability $p_3 = 1$ and the rest all zeroes. Its contribution to the loss should be zero, and it is:\n",
    "\n",
    "\\begin{equation}\n",
    "y_3 \\times \\log p_3 = 1 \\times \\log 1 = 1 \\times 0 = 0\n",
    "\\end{equation}\n",
    "\n",
    "All the other probabilities ($i \\neq 3$), being all zeroes as we desire, should also contribute zero:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i \\times \\log p_i = 0 \\times \\log 0 = 0 \\times -\\infty = 0\n",
    "\\end{equation}\n",
    "\n",
    "The total cross entropy $J$ for this one image would therefore be $-0$ or simply zero.\n",
    "\n",
    "Now, if the network answers incorrectly on this image, by assigning the probability 1 to the digit class 9 and zeroes to all other classes, the contribution of its answer to to the loss should be significant:\n",
    "\n",
    "\\begin{equation}\n",
    "y_3 \\times \\log p_3 = 1 \\times \\log 0 = 1 \\times -\\infty = -\\infty\n",
    "\\end{equation}\n",
    "\n",
    "And for the wrong answer:\n",
    "\n",
    "\\begin{equation}\n",
    "y_9 \\times \\log p_9 = 0 \\times \\log 1 = 0 \\times 0 = 0\n",
    "\\end{equation}\n",
    "\n",
    "The total cross entropy $J$ for this one image would therefore be $-(-\\infty)$ or $+\\infty$.\n",
    "\n",
    "Our loss function is now simply the mean of the losses for all the images we train with, typically in a batch. We'll then aim to minimize this mean to get better and learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "\n",
    "    xentropy = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=y,\n",
    "        logits=logits)\n",
    "\n",
    "    loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where we'll use a so-called Adam optimizer to minimize the loss function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    \n",
    "    optimizer   = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are training and learning and thereby reduce the loss on the training set. Do we also have an independent measure for how we are on _any_ data? For instance, we'd like to know how good we are on a dedicated test set. We use _accuracy_ to determine this.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "If we feed an image that represents the digit 3, for instance, then we are accurate if output neuron #3 outputs the highest value of all output neurons. Remember that we treat these as _logits_, so a highest output corresponds to the highest probability.\n",
    "\n",
    "We will use the `in_top_k(k=1)` function of Tensorflow, which will output (per image) a boolean that indicates whether the prediciton for the target class 3 is in the top `k=1` predictions that the network makes. In other words, we wanted the network to predict target class 3, and did its top prediction indicate that class?\n",
    "\n",
    "Since we are feeding batches of images at a time, this function actually returns a 1D tensor, with booleans for all images in the batch. We cast these booleans to floats (`True` becoming 1.0, and `False` becoming 0.0) and calculate the mean. The higher the mean, the more `True` values we had and the more accurate we are.\n",
    "\n",
    "Note that since `in_top_k` does not expect a one-hot encoded ground truth, but a simple integer class number, we first convert the `y` by means of the $\\arg\\max$ function, which returns the index $i$ where $y_i$ has its maximum value (of 1.0 in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    \n",
    "    labels   = tf.argmax(y, axis=1)\n",
    "    correct  = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now         = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_logs'\n",
    "logdir      = '{}/run-{}/'.format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary      = tf.summary.scalar('loss', loss)\n",
    "acc_train_summary = tf.summary.scalar('acc_train', accuracy)\n",
    "acc_test_summary  = tf.summary.scalar('acc_test',  accuracy)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 13:31:43.416863: Training...\n",
      "\r",
      "1 of 5 epochs"
     ]
    }
   ],
   "source": [
    "n_epochs   = 5\n",
    "batch_size = 100\n",
    "n_batches  = mnist.train.num_examples // batch_size\n",
    "\n",
    "print(\"{}: Training...\".format(datetime.now()))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"\\r{} of {} epochs\".format(epoch + 1, n_epochs), end='')\n",
    "\n",
    "        # Each epoch we train all batches.\n",
    "        for batch in range(n_batches):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "            # Evaluate the network now and then, so we can visualize\n",
    "            # its progress in Tensorboard.\n",
    "            if batch % 10 == 0:\n",
    "                step = epoch * n_batches + batch\n",
    "                file_writer.add_summary(\n",
    "                    loss_summary.eval(feed_dict={X: X_batch, y: y_batch}),\n",
    "                    step)\n",
    "                file_writer.add_summary(\n",
    "                    acc_train_summary.eval(feed_dict={X: X_batch, y: y_batch}),\n",
    "                    step)\n",
    "                file_writer.add_summary(\n",
    "                    acc_test_summary.eval(feed_dict={X: X_test,  y: y_test}),\n",
    "                    step)\n",
    "        \n",
    "print(\"{}: All done.\".format(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
