{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is described in detail in [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "\n",
    "In short, it returns a probability distribution for the values in a vector **z**. It is defined as, for each element $z_j$ in vector **z**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z)_{j} = \\frac{e^{z_{j}}}{\\Sigma_{k=1}^K e^{z_{k}}}\n",
    "\\end{equation}\n",
    "\n",
    "Recognize that it *scales* the exponent of each element (= $e^{z_j}$) by dividing it by a constant sum, which is the summation of the exponents of all the elements of the vector (= $\\Sigma_{k=1}^K e^{z_{k}}$).\n",
    "\n",
    "If there is just one value in **z**, it translates to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z)_{1} = \\frac{e^{z_{j}}}{\\Sigma_{k=1}^1 e^{z_{k}}} = \\frac{e^{z_1}}{e^{z_1}} = 1\n",
    "\\end{equation}\n",
    "\n",
    "For two values:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\sigma(z)_{1} &= \\frac{e^{z_1}}{e^{z_1} + e^{z_2}} \\\\\n",
    "\\sigma(z)_{2} &= \\frac{e^{z_2}}{e^{z_1} + e^{z_2}} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "And so on.\n",
    "\n",
    "Note that the sum of the softmax result is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax(vector):\n",
    "    s = 0\n",
    "    for x in vector:\n",
    "        s = s + math.exp(x)\n",
    "    return [math.exp(x) / s for x in vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2689414213699951, 0.7310585786300049]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6336913225737218,\n",
       " 0.00426977854528211,\n",
       " 0.03154963320110001,\n",
       " 0.08576079462509835,\n",
       " 0.011606461431184656,\n",
       " 0.23312200962361299]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [5, 0, 2, 3, 1, 4]\n",
    "softmax(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(softmax(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Chain**: a process with a fixed number of _states_, where an agent _randomly_ evolves from state to state:\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2014/07/transition1.png\" />\n",
    "\n",
    "**Markov Decision Process**: like a Markov Chain, except that the agent has to choose one of possible _actions_ first. The state transition probability depends on the action chosen, and some transitions may return a reward or penalty.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Markov_Decision_Process_example.png/400px-Markov_Decision_Process_example.png\" />\n",
    "\n",
    "The agent's _goal_ is to find a policy that will maximize the rewards over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal State Value** of a state $s$: the sum of all discounted future rewards the agent can expect on average after it reaches a state _s_, assuming it acts optimally.\n",
    "\n",
    "Defined in the _recursive_ Bellman Optimality Equation, for all $s$:\n",
    "\n",
    "\\begin{equation}\n",
    "V^*(s) = \\max_a \\sum_{s'} T(s,a,s') \\big[ R(s,a,s') + \\gamma \\cdot V^*(s') \\big]\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "Math term   | Meaning\n",
    "------------|:-------\n",
    "$T(s,a,s')$ | The transition probability from state $s$ to state $s'$, giving that the agent chose action $a$.\n",
    "$R(s,a,s')$ | The reward that the agent gets when it goes from state $s$ to state $s'$, given that the agent chose action $a$.\n",
    "$\\gamma$    | The discount rate.\n",
    "\n",
    "So this formula looks recursively through all actions $a$ to _maximize_ the expected future rewards, simply put.\n",
    "\n",
    "This can be found inductively, for all $s$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "V_0(s)     & \\leftarrow 0 \\\\\n",
    "V_{k+1}(s) & \\leftarrow \\max_a \\sum_{s'} T(s,a,s') \\big[ R(s,a,s') + \\gamma \\cdot V_k(s') \\big]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Now we know the optimal state _value_ $V^*(s)$, but not which action $a$ to take. That's where the next definition comes in:\n",
    "\n",
    "**Q-Value** of a state-action pair $(s,a)$: the sum of discounted future rewards the agent can expect on average after it reaches the state $s$ and chooses action $a$, but _before_ it sees the outcome of this action, assuming it acts optimally after that action.\n",
    "\n",
    "Defined in the Q-Value Iteration algorithm, very similar to the Bellman Equality Equation, for all $(s,a)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Q_0(s,a)     & \\leftarrow 0 \\\\\n",
    "Q_{k+1}(s,a) & \\leftarrow \\sum_{s'} T(s,a,s') \\big[ R(s,a,s') + \\gamma \\cdot \\max_{a'} Q_k(s',a') \\big]\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nan = np.nan  # represents impossible actions\n",
    "\n",
    "# Please reference figure 16-8 in the book.\n",
    "\n",
    "# shape = [s, a, s']\n",
    "T = np.array([\n",
    "    # ---- a0 -----    ---- a1 -----    ---- a2 -----\n",
    "    # S'0  S'1  S'2    S'0  S'1  S'2    S'0  S'1  S'2\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],  # S0\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],  # S1\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]]   # S2\n",
    "])\n",
    "\n",
    "# shape = [s, a, s']\n",
    "R = np.array([\n",
    "    # ---- a0 -----    ---- a1 -----    ---- a2 -----\n",
    "    # S'0  S'1  S'2    S'0  S'1  S'2    S'0  S'1  S'2\n",
    "    [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],   # S0\n",
    "    [[0.0, 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],  # S1\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]]    # S2\n",
    "])\n",
    "\n",
    "possible_actions = [\n",
    "    [0, 1, 2],  # S0\n",
    "    [0, 2],     # S1\n",
    "    [1]         # S2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = np.full((3, 3), -np.inf)  # -inf for impossible actions\n",
    "\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount_rate = 0.95\n",
    "n_iterations  = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s, a] = np.sum([\n",
    "                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.88646117,  20.79149867,  16.854807  ],\n",
       "       [  1.10804034,         -inf,   1.16703135],\n",
       "       [        -inf,  53.8607061 ,         -inf]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q, axis=1)  # optimal action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning algorithm**: adaption of the Q-Value Iteration algorithm where the transition probabilities and the rewards are initially unknown.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Q_0(s,a)     & \\leftarrow 0 \\\\\n",
    "Q_{k+1}(s,a) & \\leftarrow (1 - \\alpha) Q_k(s,a) + \\alpha \\big( r + \\gamma \\cdot \\max_{a'} Q_k(s',a') \\big)\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "learning_rate0      = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations        = 20000\n",
    "\n",
    "s = 0  # start in state 0\n",
    "\n",
    "Q = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    a  = rnd.choice(possible_actions[s])  # choose an action (randomly)\n",
    "    sp = rnd.choice(range(3), p=T[s, a])  # pick next state using T[s, a]\n",
    "    reward = R[s, a, sp]\n",
    "\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    \n",
    "    Q[s, a] = learning_rate * Q[s, a] + (1 - learning_rate) * (reward + discount_rate * np.max(Q[sp]))\n",
    "    \n",
    "    s = sp  # move to next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 146.52059407,  139.19456437,  113.28214669],\n",
       "       [ 113.28146025,          -inf,  115.02178305],\n",
       "       [         -inf,  179.19442723,          -inf]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
